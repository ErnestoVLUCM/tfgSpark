\chapter{Conclusiones}

Durante el presente trabajo nos hemos ido adentrando en el framework Apache Spark, con la intención no solo de aprender a usarlo, sino también de conocer sus características internas, sus distintos elementos y cómo estos se relacionan entre sí, conocimientos necesarios para optimizar nuestras aplicaciones en Spark. Con lo abordado a lo largo de  estas páginas, hemos aprendido que uno de los puntos fuertes de Spark es la canalización en las transformaciones narrow, obteniendo así una velocidad muy por encima con respecto a sus predecesores en aquellos casos en los cuales no necesitemos conocer la información ubicada en los distintos nodos del clúster, así como el rico abanico de posibilidades que nos ofrece Spark, yendo desde el machine learning hasta el procesamiento en streaming , pasando por el tratamiento de grafos. \\

Además, gracias a los conocimientos adquiridos ahora somos conscientes de los puntos delicados que tiene una aplicación Spark, destacando el cuidado que se debe tener al realizar una acción o cómo la operación shuffle influye en nuestros procesos.\\

Cabe destacar, que al mismo tiempo que hemos ido tomando contacto con Spark, nos hemos adentrado en el lenguaje Scala, que nos proporciona una acertada integración de los paradigmas de programación funcional y de la programación orientada a objetos, gracias a lo cual ha sido adoptado por muchas compañias como Twitter, LinkedIn o Sony, entre otras muchas.\\

Sin embargo, encontramos puntos en los que ahondar en esta memoria. Pese a que el objetivo principal se ha cumplido, entender los mecanismos del core de Spark, queda abierto el estudio en profundidad de las distintas librerías que componen esta herramienta, así como un tratamiento más práctico de los distintos elementos estudiados.\\

Concluyo que este proyecto ha resultado una primera toma de contacto perfecta con una herramienta tan potente, compleja y viva como Spark; espero poner en práctica todo lo aprendido en mi vida laboral , así como continuar investigando y profundizando en esta materia.\\

\section{Trabajo futuro}

Partiendo de lo abordado a lo largo de esta memoria, se podría seguir estudiando las distintas librerías de Spark, así como las mejoras introducidas con las APIs estructuradas \cite{9Catalyst} \cite{9DDCatalyst}. Además, sería interesante darle un enfoque más práctico, trabajando con un clúster, y realizar flujos de trabajo integrando herramientas adicionales como pueden ser Kafka \cite{9kafka} o ElasticSearch \cite{9elasticsearch}.
