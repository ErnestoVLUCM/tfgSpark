\chapter*{}
%\thispagestyle{plain}
\fancyhead[O]{}
\fancyfoot[O]{}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}

\section*{Resumen}

Estamos generando más datos que nunca: según \textit{World Wide Web Size} \cite{wwws}, se llegó a alcanzar en el 2016 la cifra de un Zettabyte, o, lo que es lo mismo, 1.099.511.627.776 GB. En 2017, durante cada minuto se realizaron más de 3.8 millones de búsquedas en Google, se escucharon más de 1.5 millones de canciones en Spotify, se enviaron más de 29 millones de mensajes por whatsapp, 400 horas de video se subieron a Youtube… Frente a estas cifras, el uso de las herramientas tradicionales no resulta el más apropiado para abordar análisis de datos.\\

Cuando la cantidad de información es demasiado grande para ser tratada en una sola máquina, ha de paralelizarse. Spark es un framework de procesamiento distribuido en memoria que hace uso del paradigma de programación MapReduce para realizar computación distribuida en un clúster. En esta memoria nos familiarizaremos con esta herramienta, aprenderemos cómo trabajar con Spark y cómo Spark opera. 



{\setlength{\parskip}{30mm}
\section*{Palabras clave}
}
Spark, Big data, dato, scala, hadoop, Apache, clúster, RDD, Dataset, DataFrames, Shuffle

